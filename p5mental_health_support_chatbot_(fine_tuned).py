# -*- coding: utf-8 -*-
"""P5Mental Health Support Chatbot (Fine-Tuned).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H4i74Rn8IyR5cJ4hkRwLvfVL8CUQb16f

::# **Task 5: Mental Health Support Chatbot (Fine-Tuned)**
"""

pip install -q transformers datasets torch accelerate

pip install readline

pip install nltk

pip install streamlit

!pip uninstall -y datasets
!pip install datasets==2.14.5

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

from datasets import load_dataset

dataset = load_dataset("facebook/empathetic_dialogues")

print(dataset)

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_name)

def preprocess_for_empathy(batch):
    input_ids = []
    attention_masks = []
    labels = []

    for context, utterance in zip(batch["context"], batch["utterance"]):
        # Convert list of turns into a single string
        context_text = " ".join(context)

        # Gentle & empathetic prompt format
        prompt = f"Context: {context_text}\nResponse:"
        full_text = prompt + " " + utterance

        tokenized = tokenizer(
            full_text,
            truncation=True,
            padding="max_length",
            max_length=96
        )

        input_ids.append(tokenized["input_ids"])
        attention_masks.append(tokenized["attention_mask"])
        labels.append(tokenized["input_ids"].copy())  # causal LM

    return {
        "input_ids": input_ids,
        "attention_masks": attention_masks,
        "labels": labels
    }

tokenized_dataset = dataset.map(
    preprocess_for_empathy,
    batched=True)

print(tokenized_dataset)

import torch
from transformers import Trainer, TrainingArguments
import os
os.environ["WANDB_DISABLED"] = "true"

training_args = TrainingArguments(
    output_dir="./empathetic_chatbot",
    overwrite_output_dir=True,
    eval_strategy="steps", # Changed from evaluation_strategy
    eval_steps=500,
    save_strategy="steps",
    save_steps=500,
    save_total_limit=2,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    learning_rate=5e-5,
    fp16=False,
    logging_steps=100
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset = tokenized_dataset["train"].shuffle(seed=42).select(range(5000)),
    eval_dataset = tokenized_dataset["validation"].shuffle(seed=42).select(range(1000)),
    tokenizer=tokenizer
)


trainer.train()

results = trainer.evaluate()
print(results)

model.save_pretrained("./empathetic_chatbot/final_model")
tokenizer.save_pretrained("./empathetic_chatbot/final_model")

import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./empathetic_chatbot/final_model")
model = AutoModelForCausalLM.from_pretrained("./empathetic_chatbot/final_model")

st.title("ðŸ§ ðŸ’¬ Mental Health Support Chatbot")

user_input = st.text_input("How are you feeling today?")

if user_input:
    prompt = f"Context: {user_input}\nResponse:"
    inputs = tokenizer(prompt, return_tensors="pt")

    output = model.generate(
        **inputs,
        max_length=120,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

    response = tokenizer.decode(output[0], skip_special_tokens=True)
    st.write("**Bot:**", response)